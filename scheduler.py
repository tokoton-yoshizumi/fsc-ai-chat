# scheduler.py (æ–°ã—ã„å®Œæˆç‰ˆã‚³ãƒ¼ãƒ‰)

import requests
import json
import re
from bs4 import BeautifulSoup
import schedule
import time
from threading import Thread

# --- create_database.py ã‹ã‚‰æŒã£ã¦ããŸãƒ­ã‚¸ãƒƒã‚¯ ---

PAGES_API_URL = "https://fujiwarasangyo.jp/wp-json/wp/v2/pages?per_page=100"
POSTS_API_URL = "https://fujiwarasangyo.jp/wp-json/wp/v2/posts?per_page=100"


EXCLUDE_URLS = [
    "https://fujiwarasangyo.jp/", # ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸
    "https://fujiwarasangyo.jp/form-contact/",
    "https://fujiwarasangyo.jp/order/",
    "https://fujiwarasangyo.jp/rockbolt-form/",
    "https://fujiwarasangyo.jp/form-rental/",
    "https://fujiwarasangyo.jp/calibratio-repair/",
    "https://fujiwarasangyo.jp/number-view/"
    "https://fujiwarasangyo.jp/%e3%83%88%e3%83%83%e3%83%97%e3%83%9a%e3%83%bc%e3%82%b8/"
    
    # ä»–ã«ã‚‚é™¤å¤–ã—ãŸã„ãƒšãƒ¼ã‚¸ãŒã‚ã‚Œã°ã“ã“ã«è¿½åŠ 
]
def clean_html(html_content):
    soup = BeautifulSoup(html_content, "html.parser")
    text = soup.get_text()
    text = re.sub(r'\s{2,}', '\n', text).strip()
    return text

def fetch_all_content(api_url):
    all_items = []
    page = 1
    while True:
        try:
            response = requests.get(f"{api_url}&page={page}", timeout=20)
            response.raise_for_status()
            items = response.json()
            if not items:
                break
            all_items.extend(items)
            page += 1
        except requests.exceptions.RequestException as e:
            print(f"âŒ APIã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            break
    return all_items

def update_database():
    """ã‚µã‚¤ãƒˆã®å…¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ã—ã¦JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°ã™ã‚‹"""
    print("ğŸ”„ [ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼] ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®å®šæœŸæ›´æ–°ã‚’é–‹å§‹ã—ã¾ã™...")
    
    pages = fetch_all_content(PAGES_API_URL)
    posts = fetch_all_content(POSTS_API_URL)
    all_content = pages + posts
    
    site_data = []
    for item in all_content:
        url = item.get('link', '')
        if url in EXCLUDE_URLS:
            print(f"ğŸš« ã‚¹ã‚­ãƒƒãƒ— (é™¤å¤–ãƒªã‚¹ãƒˆ): {item.get('title', {}).get('rendered', '')}")
            continue
        if 'content' in item and 'rendered' in item['content'] and item['content']['rendered']:
            title = item.get('title', {}).get('rendered', 'No Title')
            html_content = item['content']['rendered']
            cleaned_content = clean_html(html_content)
            site_data.append({"title": title, "url": url, "content": cleaned_content})

    with open("site_content.json", "w", encoding="utf-8") as f:
        json.dump(site_data, f, ensure_ascii=False, indent=2)

    print(f"âœ… [ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼] å…¨{len(site_data)}ãƒšãƒ¼ã‚¸ã®ãƒ‡ãƒ¼ã‚¿ã§ `site_content.json` ã‚’æ›´æ–°ã—ã¾ã—ãŸã€‚")

# --- ã“ã“ã¾ã§ãŒ create_database.py ã®ãƒ­ã‚¸ãƒƒã‚¯ ---


# **ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã®è¨­å®š**
# æ¯é€±æœˆæ›œæ—¥ã®æ—©æœ3æ™‚ã«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ›´æ–°ã™ã‚‹ã‚ˆã†ã«è¨­å®š
schedule.every().monday.at("03:00").do(update_database)


def start_scheduler():
    """ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å®Ÿè¡Œ"""
    # â˜… ã‚µãƒ¼ãƒãƒ¼èµ·å‹•æ™‚ã«ä¸€åº¦ã ã‘å³æ™‚å®Ÿè¡Œã™ã‚‹
    # print("ğŸš€ [åˆå›å®Ÿè¡Œ] ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æœ€æ–°ã®çŠ¶æ…‹ã«æ›´æ–°ã—ã¾ã™...")
    # update_database()
    
    def run_scheduler():
        while True:
            schedule.run_pending()
            time.sleep(1) # 1ç§’ã”ã¨ã«æ¬¡ã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ç¢ºèª

    Thread(target=run_scheduler, daemon=True).start()