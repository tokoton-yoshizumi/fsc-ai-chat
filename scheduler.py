import requests
import json
import xml.etree.ElementTree as ET
import schedule
import time
from threading import Thread
from utils import load_existing_data, get_page_title

PAGE_SITEMAP_URL = "https://fujiwarasangyo.jp/page-sitemap.xml"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

def get_page_urls(sitemap_url):
    """Sitemap ã‹ã‚‰å›ºå®šãƒšãƒ¼ã‚¸ã® URL ã‚’å–å¾—"""
    try:
        response = requests.get(sitemap_url, headers=HEADERS, timeout=10)
        response.raise_for_status()
        root = ET.fromstring(response.text)
        namespaces = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}
        urls = [elem.text for elem in root.findall(".//ns:loc", namespaces)]
        return urls
    except requests.exceptions.RequestException as e:
        print(f"âŒ Sitemap ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return []

def update_sitemap():
    """é€±ã«1å› sitemap ã‚’å–å¾—ã—ã¦ JSON ã‚’æ›´æ–°"""
    print("ğŸ”„ Sitemap ã‹ã‚‰æœ€æ–°ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
    
    fixed_page_urls = get_page_urls(PAGE_SITEMAP_URL)
    existing_page_data = load_existing_data("fixed_page_titles.json")
    existing_urls = {entry["url"] for entry in existing_page_data}

    new_page_data = []
    total_pages = len(fixed_page_urls)
    print(f"ğŸ“Œ å–å¾—å¯¾è±¡ã®å›ºå®šãƒšãƒ¼ã‚¸æ•°: {total_pages}")

    for index, url in enumerate(fixed_page_urls, start=1):
        if url in existing_urls:
            print(f"[{index}/{total_pages}] {url} ã¯å–å¾—æ¸ˆã¿ âœ… ã‚¹ã‚­ãƒƒãƒ—")
            continue

        print(f"[{index}/{total_pages}] {url} ã‚’å‡¦ç†ä¸­...", end=" ")
        title = get_page_title(url)
        if title:
            new_page_data.append({"url": url, "title": title})
            print("âœ… å–å¾—æˆåŠŸ")
        else:
            print("âŒ å–å¾—å¤±æ•—")

    # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã«æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
    updated_page_data = existing_page_data + new_page_data

    # JSON ã«ä¿å­˜
    with open("fixed_page_titles.json", "w", encoding="utf-8") as f:
        json.dump(updated_page_data, f, ensure_ascii=False, indent=4)

    print("âœ… æœ€æ–°ã®ãƒ‡ãƒ¼ã‚¿ã«æ›´æ–°ã—ã¾ã—ãŸï¼")

# **ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã®è¨­å®š**
schedule.every().week.do(update_sitemap)

def start_scheduler():
    """ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å®Ÿè¡Œ"""
    def run_scheduler():
        while True:
            schedule.run_pending()
            time.sleep(60)

    Thread(target=run_scheduler, daemon=True).start()
