import os
import json
import requests
from bs4 import BeautifulSoup
import re


# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨­å®š
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

def load_existing_data(filename):
    """æ—¢å­˜ã®ã‚¿ã‚¤ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰"""
    if os.path.exists(filename):
        with open(filename, "r", encoding="utf-8") as f:
            try:
                return json.load(f)
            except json.JSONDecodeError:
                return []
    return []

def fetch_page_headings(url):
    """æŒ‡å®šã—ãŸãƒšãƒ¼ã‚¸ã® h2 è¦‹å‡ºã—ã‚’å–å¾—ã™ã‚‹"""
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, "html.parser")
        headings = [h.get_text().strip() for h in soup.find_all("h2")]

        return headings if headings else []
    except requests.exceptions.RequestException as e:
        print(f"âŒ è¦‹å‡ºã—å–å¾—å¤±æ•— ({url}): {e}")
        return []


def fetch_section_content(url, heading):
    """æŒ‡å®šã—ãŸ `h2` ã®ä¸‹ã«ã‚ã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ã—ã€ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, "html.parser")
        headings = soup.find_all("h2")  # h2 ã®ã¿ã‚’å–å¾—

        safe_heading = re.sub(r'[^a-zA-Z0-9_]', '', heading.replace(' ', '_'))
        filename = f"output_{safe_heading}.txt"

        if not safe_heading:
            filename = "output_default.txt" # ã‚‚ã—ãƒ•ã‚¡ã‚¤ãƒ«åãŒç©ºã«ãªã£ãŸå ´åˆã®å¯¾ç­–

        print(f"ğŸ” {url} ã® h2 è¦‹å‡ºã—ãƒªã‚¹ãƒˆ:")
        for h in headings:
            print(f" - {h.get_text().strip()}")

        for h in headings:
            h_text = h.get_text().strip()
            
            # **å¤§æ–‡å­—å°æ–‡å­—ã‚’ç„¡è¦–ã—ã€å®Œå…¨ä¸€è‡´ã‚’ç¢ºèª**
            if heading.lower().strip() == h_text.lower():
                print(f"âœ… ä¸€è‡´ã—ãŸè¦‹å‡ºã—: {h_text}")

                content = []
                next_elements = h.find_all_next()  # h2 ä»¥é™ã®ã™ã¹ã¦ã®è¦ç´ ã‚’å–å¾—

                for elem in next_elements:
                    if elem.name == "h2":  # æ¬¡ã® h2 ã§çµ‚äº†
                        break
                    if elem.name in ["p", "ul", "li", "table"]:
                        content.append(elem.get_text().strip())

                result = "\n".join(content) if content else "è©²å½“ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
                print(f"ğŸ”¹ å–å¾—ã—ãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„: {result[:100]}...")  # 100æ–‡å­—ã ã‘è¡¨ç¤º

                # **ãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›**
                with open(filename, "w", encoding="utf-8") as f:
                    f.write(f"ã€{heading}ã€‘\n{result}")

                print(f"âœ… å‡ºåŠ›å®Œäº†: {filename}")  # å‡ºåŠ›ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«åã‚’è¡¨ç¤º

                return result

        print("âŒ ä¸€è‡´ã™ã‚‹è¦‹å‡ºã—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return "è©²å½“ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
    except requests.exceptions.RequestException as e:
        print(f"âŒ ã‚»ã‚¯ã‚·ãƒ§ãƒ³å–å¾—å¤±æ•— ({url}): {e}")
        return "ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ"

def get_page_title(url):
    """æŒ‡å®šã—ãŸ URL ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—"""
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, "html.parser")
        title = soup.title.string.strip() if soup.title else "No Title"

        return title
    except requests.exceptions.RequestException as e:
        print(f"âŒ ã‚¿ã‚¤ãƒˆãƒ«å–å¾—å¤±æ•— ({url}): {e}")
        return None
