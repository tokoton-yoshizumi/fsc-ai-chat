import requests
import json
import re
from bs4 import BeautifulSoup

# WordPressã‚µã‚¤ãƒˆã®APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
# NOTE: per_page=100 ã¨ã™ã‚‹ã“ã¨ã§ã€ä¸€åº¦ã«100ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€ãƒªã‚¯ã‚¨ã‚¹ãƒˆå›æ•°ã‚’æ¸›ã‚‰ã—ã¾ã™ã€‚
# ãƒšãƒ¼ã‚¸æ•°ãŒ100ã‚’è¶…ãˆã‚‹å ´åˆã¯ã€ãƒ«ãƒ¼ãƒ—å‡¦ç†ã§å…¨ãƒšãƒ¼ã‚¸ã‚’å–å¾—ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
PAGES_API_URL = "https://fujiwarasangyo.jp/wp-json/wp/v2/pages?per_page=100"
POSTS_API_URL = "https://fujiwarasangyo.jp/wp-json/wp/v2/posts?per_page=100"

EXCLUDE_URLS = [
    "https://fujiwarasangyo.jp/", # ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸
    "https://fujiwarasangyo.jp/form-contact/",
    "https://fujiwarasangyo.jp/order/",
    "https://fujiwarasangyo.jp/rockbolt-form/",
    "https://fujiwarasangyo.jp/form-rental/",
    "https://fujiwarasangyo.jp/calibratio-repair/",
    


    # ä»–ã«ã‚‚é™¤å¤–ã—ãŸã„ãƒšãƒ¼ã‚¸ãŒã‚ã‚Œã°ã“ã“ã«è¿½åŠ 
]

def clean_html(html_content):
    """HTMLã‚¿ã‚°ã‚’é™¤å»ã—ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ³ã«ã™ã‚‹"""
    soup = BeautifulSoup(html_content, "html.parser")
    text = soup.get_text()
    # é€£ç¶šã™ã‚‹æ”¹è¡Œã‚„ç©ºç™½ã‚’æ•´ç†
    text = re.sub(r'\s{2,}', '\n', text).strip()
    return text

def fetch_all_content(api_url):
    """æŒ‡å®šã•ã‚ŒãŸAPIã‹ã‚‰ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ã™ã‚‹"""
    all_items = []
    page = 1
    while True:
        try:
            response = requests.get(f"{api_url}&page={page}", timeout=20)
            response.raise_for_status()
            items = response.json()
            if not items:
                break # ãƒ‡ãƒ¼ã‚¿ãŒãªããªã£ãŸã‚‰ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹
            all_items.extend(items)
            page += 1
        except requests.exceptions.RequestException as e:
            print(f"âŒ APIã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            break
    return all_items

def create_content_database():
    """ã‚µã‚¤ãƒˆã®å…¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ã—ã¦JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹"""
    print("ğŸ”„ ã‚µã‚¤ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å–å¾—ã‚’é–‹å§‹ã—ã¾ã™...")

    pages = fetch_all_content(PAGES_API_URL)
    posts = fetch_all_content(POSTS_API_URL)
    
    all_content = pages + posts
    
    site_data = []

    for item in all_content:
        url = item.get('link', '')
        
        # â˜…â˜…â˜… é™¤å¤–ãƒªã‚¹ãƒˆã«å«ã¾ã‚Œã‚‹URLã¯ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹å‡¦ç†ã‚’è¿½åŠ  â˜…â˜…â˜…
        if url in EXCLUDE_URLS:
            print(f"ğŸš« ã‚¹ã‚­ãƒƒãƒ— (é™¤å¤–ãƒªã‚¹ãƒˆ): {item.get('title', {}).get('rendered', '')}")
            continue

        if 'content' in item and 'rendered' in item['content'] and item['content']['rendered']:
            title = item.get('title', {}).get('rendered', 'No Title')
            html_content = item['content']['rendered']
            cleaned_content = clean_html(html_content)

            site_data.append({
                "title": title,
                "url": url,
                "content": cleaned_content
            })
            print(f"âœ… å–å¾—å®Œäº†: {title}")

    # JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    with open("site_content.json", "w", encoding="utf-8") as f:
        json.dump(site_data, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… å…¨{len(site_data)}ãƒšãƒ¼ã‚¸ã®ãƒ‡ãƒ¼ã‚¿ã‚’ `site_content.json` ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

if __name__ == "__main__":
    create_content_database()
